io:
  dataset_prefix: CardinalityLM/imdb-card-pred-
  mode: decimal
  output_dir: /content/ppo_snapshots-gpt2
  run_tag: yuanbiao-colab-ppo-gpt2
  sft_checkpoint: /content/final_checkpoint/
  use_bf16: true
  device_map: "cuda:0"
training:
  max_steps: 120
  gradient_accumulation_steps: 2
  learning_rate: 0.0002
  batch_size: 4
  optimizer: "adamw_torch"
model:
  use_bf16: false
  use_flash_attention2: false
  model_name: gpt2
  adapter: lora
misc:
  seed: 42
  token: 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
inference:
  max_length: 16